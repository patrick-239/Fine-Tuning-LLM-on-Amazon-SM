{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7d83863-427f-4585-87d7-f19ee4fc7470",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Contextual chatbot using Llama2 via SageMaker JumpStart and Amazon OpenSearch Serverless with Vector Engine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd59e781-e006-4494-abfe-47050d0611e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "You will need these prerequisites in order to build the following context aware chatbot:\n",
    "- An Amazon SageMaker Execution Role with IAM permission to access Amazon OpenSearch Serverless (aoss). The [in-line](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_manage-attach-detach.html#add-policies-console) policy can be found [here](./IAM/sagemaker-execution-role-aoss-policy.yml). You can determine the correct SageMaker Role by running the `setup sagemaker session` cell below.\n",
    "- 2 x ml.g5.2xlarge instance types for SageMaker Endpoints. Instruction on increasing your service quota can be found [here](https://aws.amazon.com/getting-started/hands-on/request-service-quota-increase/)."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Context\n",
    "Previously we saw that the model told us how to to change the tire, however we had to manually provide it with the relevant data and provide the contex ourselves. We explored the approach to leverage the model availabe under Bedrock and ask questions based on it's knowledge learned during training as well as providing manual context. While that approach works with short documents or single-ton applications, it fails to scale to enterprise level question answering where there could be large enterprise documents which cannot all be fit into the prompt sent to the model. \n",
    "\n",
    "### Pattern\n",
    "We can improve upon this process by implementing an architecure called Retreival Augmented Generation (RAG). RAG retrieves data from outside the language model (non-parametric) and augments the prompts by adding the relevant retrieved data in context. \n",
    "\n",
    "In this notebook we explain how to approach the pattern of Question Answering to find and leverage the documents to provide answers to the user questions.\n",
    "\n",
    "### Challenges\n",
    "- How to manage large document(s) that exceed the token limit\n",
    "- How to find the document(s) relevant to the question being asked\n",
    "\n",
    "### Proposal\n",
    "To the above challenges, this notebook proposes the following strategy\n",
    "#### Prepare documents\n",
    "![Embeddings](./images/Embeddings_lang.png)\n",
    "\n",
    "Before being able to answer the questions, the documents must be processed and a stored in a document store index\n",
    "- Load the documents\n",
    "- Process and split them into smaller chunks\n",
    "- Create a numerical vector representation of each chunk using Amazon Bedrock Titan Embeddings model\n",
    "- Create an index using the chunks and the corresponding embeddings\n",
    "#### Ask question\n",
    "![Question](./images/Chatbot_lang.png)\n",
    "\n",
    "When the documents index is prepared, you are ready to ask the questions and relevant documents will be fetched based on the question being asked. Following steps will be executed.\n",
    "- Create an embedding of the input question\n",
    "- Compare the question embedding with the embeddings in the index\n",
    "- Fetch the (top N) relevant document chunks\n",
    "- Add those chunks as part of the context in the prompt\n",
    "- Send the prompt to the model under Amazon Bedrock\n",
    "- Get the contextual answer based on the documents retrieved\n",
    "\n",
    "## Usecase\n",
    "#### Dataset\n",
    "To explain this architecture pattern we are using the documents from IRS. These documents explain topics such as:\n",
    "- Original Issue Discount (OID) Instruments\n",
    "- Reporting Cash Payments of Over $10,000 to IRS\n",
    "- Employer's Tax Guide\n",
    "\n",
    "#### Persona\n",
    "Let's assume a persona of a layman who doesn't have an understanding of how IRS works and if some actions have implications or not.\n",
    "\n",
    "The model will try to answer from the documents in easy language.\n",
    "\n",
    "\n",
    "## Implementation\n",
    "In order to follow the RAG approach this notebook is using the LangChain framework where it has integrations with different services and tools that allow efficient building of patterns such as RAG. We will be using the following tools:\n",
    "\n",
    "- **LLM (Large Language Model)**: Meta Llama2 available through Amazon SageMaker Jumpstart\n",
    "\n",
    "  This model will be used to understand the document chunks and provide an answer in human friendly manner.\n",
    "- **Embeddings Model**: GPT-J-6B Embeddings available through Amazon SageMaker Jumpstart\n",
    "\n",
    "  This model will be used to generate a numerical representation of the textual documents\n",
    "- **Document Loader**: PDF Loader available through LangChain\n",
    "\n",
    "  This is the loader that can load the documents from a source, for the sake of this notebook we are loading the sample files from a local path. This could easily be replaced with a loader to load documents from enterprise internal systems.\n",
    "\n",
    "- **Vector Store**: OpenSearch available through LangChain\n",
    "\n",
    "  In this notebook we are using Amazon OpenSearch as a vector-store to store both the embeddings and the documents. \n",
    "- **Index**: VectorIndex\n",
    "\n",
    "  The index helps to compare the input embedding and the document embeddings to find relevant document\n",
    "- **Wrapper**: wraps index, vector store, embeddings model and the LLM to abstract away the logic from the user."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ce24ecf9fb8cbe3"
  },
  {
   "cell_type": "markdown",
   "id": "973d8c09-007d-4c00-86f6-6d26932d1d88",
   "metadata": {},
   "source": [
    "## Install Required Python Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88c459e-6031-4bda-a882-905f8491ee68",
   "metadata": {
    "tags": []
   },
   "source": [
    "    **IMPORTANT**\n",
    "    1. Ensure you are running Pythin 3.10+\n",
    "    1. Ensure you are using the Data Science 3.0 kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22ee7fd-963d-4afc-8722-ec0911c4f340",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### Require python 3.10+\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3476b4c-559b-4da1-b524-14ca011e04f9",
   "metadata": {},
   "source": [
    "Begin by installing the required python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ae7b8b-0e21-4152-9538-e858b31ba271",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -U pip --quiet\n",
    "%pip install --upgrade sagemaker --quiet \n",
    "%pip install langchain --quiet\n",
    "%pip install opensearch-py --quiet\n",
    "%pip install regex --quiet\n",
    "%pip install tqdm --quiet\n",
    "%pip install requests_aws4auth --quiet\n",
    "%pip install pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb06ec0-d912-4a51-8b2e-8c898dd4041d",
   "metadata": {},
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c5c149-aa23-496e-b036-10ffc73e5615",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup SageMaker Session\n",
    "import sagemaker, boto3, json\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "\n",
    "sagemaker_session = Session()\n",
    "sm_execution_role = get_execution_role()\n",
    "\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "sagemaker_client = boto3.client('sagemaker', region_name=aws_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3e92ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import langchain \n",
    "from langchain.document_loaders import UnstructuredHTMLLoader,BSHTMLLoader,PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter,CharacterTextSplitter\n",
    "from langchain.llms.sagemaker_endpoint import ContentHandlerBase\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from langchain import LLMChain\n",
    "from langchain import SagemakerEndpoint\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "import os\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbf0e14-5253-466c-a7e2-deb5bc3cd085",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_endpoint_name = \"PUT YOUR ENDPOINT NAME HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3db3429-1109-4033-a79b-51134c8285cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings_endpoint_name = \"PUT YOUR ENDPOINT NAME HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60103805-e228-49cc-95d3-e802b0d451a7",
   "metadata": {},
   "source": [
    "Firt of all we have to create a vector store. In this workshop we will use Amazon OpenSerach serverless.\n",
    "\n",
    "Amazon OpenSearch Serverless is a serverless option in Amazon OpenSearch Service. As a developer, you can use OpenSearch Serverless to run petabyte-scale workloads without configuring, managing, and scaling OpenSearch clusters. You get the same interactive millisecond response times as OpenSearch Service with the simplicity of a serverless environment. Pay only for what you use by automatically scaling resources to provide the right amount of capacity for your application—without impacting data ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af133b35-95fe-4ae1-bf09-7341188764c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "vector_store_name = 'bedrock-workshop-rag'\n",
    "index_name = \"bedrock-workshop-rag-index\"\n",
    "encryption_policy_name = \"bedrock-workshop-rag-sp\"\n",
    "network_policy_name = \"bedrock-workshop-rag-np\"\n",
    "access_policy_name = 'bedrock-workshop-rag-ap'\n",
    "identity = boto3.client('sts').get_caller_identity()['Arn']\n",
    "\n",
    "aoss_client = boto3.client('opensearchserverless')\n",
    "\n",
    "security_policy = aoss_client.create_security_policy(\n",
    "    name = encryption_policy_name,\n",
    "    policy = json.dumps(\n",
    "        {\n",
    "            'Rules': [{'Resource': ['collection/' + vector_store_name],\n",
    "            'ResourceType': 'collection'}],\n",
    "            'AWSOwnedKey': True\n",
    "        }),\n",
    "    type = 'encryption'\n",
    ")\n",
    "\n",
    "network_policy = aoss_client.create_security_policy(\n",
    "    name = network_policy_name,\n",
    "    policy = json.dumps(\n",
    "        [\n",
    "            {'Rules': [{'Resource': ['collection/' + vector_store_name],\n",
    "            'ResourceType': 'collection'}],\n",
    "            'AllowFromPublic': True}\n",
    "        ]),\n",
    "    type = 'network'\n",
    ")\n",
    "\n",
    "collection = aoss_client.create_collection(name=vector_store_name,type='VECTORSEARCH')\n",
    "\n",
    "while True:\n",
    "    status = aoss_client.list_collections(collectionFilters={'name':vector_store_name})['collectionSummaries'][0]['status']\n",
    "    if status in ('ACTIVE', 'FAILED'): break\n",
    "    time.sleep(10)\n",
    "\n",
    "access_policy = aoss_client.create_access_policy(\n",
    "    name = access_policy_name,\n",
    "    policy = json.dumps(\n",
    "        [\n",
    "            {\n",
    "                'Rules': [\n",
    "                    {\n",
    "                        'Resource': ['collection/' + vector_store_name],\n",
    "                        'Permission': [\n",
    "                            'aoss:CreateCollectionItems',\n",
    "                            'aoss:DeleteCollectionItems',\n",
    "                            'aoss:UpdateCollectionItems',\n",
    "                            'aoss:DescribeCollectionItems'],\n",
    "                        'ResourceType': 'collection'\n",
    "                    },\n",
    "                    {\n",
    "                        'Resource': ['index/' + vector_store_name + '/*'],\n",
    "                        'Permission': [\n",
    "                            'aoss:CreateIndex',\n",
    "                            'aoss:DeleteIndex',\n",
    "                            'aoss:UpdateIndex',\n",
    "                            'aoss:DescribeIndex',\n",
    "                            'aoss:ReadDocument',\n",
    "                            'aoss:WriteDocument'],\n",
    "                        'ResourceType': 'index'\n",
    "                    }],\n",
    "                'Principal': [identity],\n",
    "                'Description': 'Easy data policy'}\n",
    "        ]),\n",
    "    type = 'data'\n",
    ")\n",
    "\n",
    "host = collection['createCollectionDetail']['id'] + '.' + os.environ.get(\"AWS_DEFAULT_REGION\", None) + '.aoss.amazonaws.com:443'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5166d759",
   "metadata": {},
   "source": [
    "## Chunk your Data and Load into Amazon OpenSearch\n",
    "\n",
    "In this section we will chunk the data into smaller documents. Chunking is a technique for splitting large texts into smaller chunks. It is an important step as it optimizes the relevance of the search query for our RAG-model. Which in turn improves the quality of the chatbot. The chunk size is dependent on factors such as the document type and model used. We have selected a `chunk_size=2000` as this is the approximate size of a paragraph. As models improve, their context window size will increase which will allow for larger chunk sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "files = [\n",
    "    \"https://www.irs.gov/pub/irs-pdf/p1544.pdf\",\n",
    "    \"https://www.irs.gov/pub/irs-pdf/p15.pdf\",\n",
    "    \"https://www.irs.gov/pub/irs-pdf/p1212.pdf\",\n",
    "]\n",
    "for url in files:\n",
    "    file_path = os.path.join(\"data\", url.rpartition(\"/\")[2])\n",
    "    urlretrieve(url, file_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "362b889f3a2d2fae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a2d6bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = PyPDFDirectoryLoader(\"./data/\")\n",
    "documents = loader.load()\n",
    "# - in our testing Character split works better with this PDF data set\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3c6f02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "avg_doc_length = lambda documents: sum([len(doc.page_content) for doc in documents]) // len(\n",
    "    documents\n",
    ")\n",
    "avg_char_count_pre = avg_doc_length(documents)\n",
    "avg_char_count_post = avg_doc_length(docs)\n",
    "print(f\"Average length among {len(documents)} documents loaded is {avg_char_count_pre} characters.\")\n",
    "print(f\"After the split we have {len(docs)} documents more than the original {len(documents)}.\")\n",
    "print(\n",
    "    f\"Average length among {len(docs)} documents (after split) is {avg_char_count_post} characters.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b5954f-fc77-45c7-9cb9-b7fc2fe24390",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper function to process document\n",
    "\n",
    "import regex as re\n",
    "\n",
    "def postproc(s):\n",
    "    s = s.replace(u'\\xa0', u' ') # no-break space \n",
    "    s = s.replace('\\n', ' ') # new-line\n",
    "    s = re.sub(r'\\s+', ' ', s) # multiple spaces\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725f1429-c9c3-49b1-a779-eda1c901232e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    doc.page_content = postproc(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28da307a",
   "metadata": {},
   "source": [
    "In the next step, we simply validate that document was chunked correctly by manually reviewing the first chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10df400-95c0-41ba-a5cf-097af738d30a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Review the first document for correctness\n",
    "docs[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0c0820-0080-4676-94a3-38594470ebd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Limit the number of total chunks to 1000\n",
    "MAX_DOCS = 1000\n",
    "if len(docs) > MAX_DOCS:\n",
    "    docs = docs[:MAX_DOCS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865a4079-5d48-4fa3-8bcc-9ba7a937102b",
   "metadata": {},
   "source": [
    "Next we extend the LangChain `SageMakerEndpointEmbeddings` Class to create a custom embeddings function that uses the `gpt-j-6b-fp16` SageMaker Endpoint you created earlier in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635f15ca-e6ec-4f4c-af9f-0a455cba1f8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import logging\n",
    "from typing import List\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandler\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# extend the SagemakerEndpointEmbeddings class from langchain to provide a custom embedding function\n",
    "class SagemakerEndpointEmbeddingsJumpStart(SagemakerEndpointEmbeddings):\n",
    "    def embed_documents(\n",
    "        self, texts: List[str], chunk_size: int = 1\n",
    "    ) -> List[List[float]]:\n",
    "        \"\"\"Compute doc embeddings using a SageMaker Inference Endpoint.\n",
    "\n",
    "        Args:\n",
    "            texts: The list of texts to embed.\n",
    "            chunk_size: The chunk size defines how many input texts will\n",
    "                be grouped together as request. If None, will use the\n",
    "                chunk size specified by the class.\n",
    "\n",
    "        Returns:\n",
    "            List of embeddings, one for each text.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        _chunk_size = len(texts) if chunk_size > len(texts) else chunk_size\n",
    "        st = time.time()\n",
    "        for i in range(0, len(texts), _chunk_size):\n",
    "            response = self._embedding_func(texts[i:i + _chunk_size])\n",
    "            results.extend(response)\n",
    "        time_taken = time.time() - st\n",
    "        logger.info(f\"got results for {len(texts)} in {time_taken}s, length of embeddings list is {len(results)}\")\n",
    "        print(f\"got results for {len(texts)} in {time_taken}s, length of embeddings list is {len(results)}\")\n",
    "        return results\n",
    "\n",
    "\n",
    "# class for serializing/deserializing requests/responses to/from the embeddings model\n",
    "class ContentHandler(EmbeddingsContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "\n",
    "        input_str = json.dumps({\"text_inputs\": prompt, **model_kwargs})\n",
    "        return input_str.encode('utf-8') \n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        embeddings = response_json[\"embedding\"]\n",
    "        if len(embeddings) == 1:\n",
    "            return [embeddings[0]]\n",
    "        return embeddings\n",
    "    \n",
    "\n",
    "def create_sagemaker_embeddings_from_js_model(embeddings_endpoint_name: str, aws_region: str) -> SagemakerEndpointEmbeddingsJumpStart:\n",
    "    # all set to create the objects for the ContentHandler and \n",
    "    # SagemakerEndpointEmbeddingsJumpStart classes\n",
    "    content_handler = ContentHandler()\n",
    "\n",
    "    # note the name of the LLM Sagemaker endpoint, this is the model that we would\n",
    "    # be using for generating the embeddings\n",
    "    embeddings = SagemakerEndpointEmbeddingsJumpStart( \n",
    "        endpoint_name=embeddings_endpoint_name,\n",
    "        region_name=aws_region, \n",
    "        content_handler=content_handler\n",
    "    )\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838b2843",
   "metadata": {},
   "source": [
    "We create the embeddings object and batch the creation of the document embeddings. These embeddinga are stored in Amazon OpenSearch using LangChain `OpenSearchVectorSearch`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed872e9-387d-4d60-8567-4866ed365677",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings = create_sagemaker_embeddings_from_js_model(embeddings_endpoint_name, aws_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8899dc-b360-4db7-9139-04d2bb18872a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "from requests_aws4auth import AWS4Auth\n",
    "from opensearchpy import RequestsHttpConnection\n",
    "\n",
    "credentials = boto3.Session().get_credentials()\n",
    "\n",
    "service=\"aoss\"\n",
    "region=aws_region\n",
    "\n",
    "awsauth = AWS4Auth(\n",
    "    credentials.access_key,\n",
    "    credentials.secret_key,\n",
    "    region,\n",
    "    service,\n",
    "    session_token=credentials.token\n",
    ")\n",
    "\n",
    "docsearch = OpenSearchVectorSearch.from_texts(\n",
    "    texts = [d.page_content for d in docs],\n",
    "    embedding=embeddings,\n",
    "    opensearch_url=host,\n",
    "    http_auth=awsauth,\n",
    "    timeout = 300,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    index_name=index_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cad6b8-aabb-41ec-b029-2809539ce783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load.dump import dumps\n",
    "\n",
    "query = \"Is it possible that I get sentenced to jail due to failure in filings?\"\n",
    "\n",
    "results = docsearch.similarity_search(query, k=3)  # our search query  # return 3 most relevant docs\n",
    "print(dumps(results, pretty=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c35dcd9",
   "metadata": {},
   "source": [
    "## Question Answering Over Documents \n",
    "\n",
    "So far, we have chunked a large document into smaller ones, created vector embedding and stored them in an Amazon OpenSearch Serverless with Vector engine. Now, we can answer questions regarding this document data.\n",
    "\n",
    "Since we have created an index over the data, we can do a semantic search; this way only the most relevant documents required to answer the question are passed via the prompt to the Large Language Model (LLM). This allows you to save both time and money by not passing all the documents to the LLM.\n",
    "\n",
    "The LLM used is `Llama2` via the SageMaker Endpoint created earlier.\n",
    "\n",
    "We use LangChian **question_answering** `stuff` document chain type in this example. Further details on Document Chains can be found by visiting the [LangChain documentation, here](https://python.langchain.com/docs/modules/chains/document/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610452c8-7b70-49c8-9f92-9a74c45be1bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from langchain import PromptTemplate, SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "import json\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "query = \"Is it possible that I get sentenced to jail due to failure in filings?\"\n",
    "#query = \"What is a personal income tax rate?\"\n",
    "\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the following QUESTION based on the CONTEXT given. If you do not know the answer and the CONTEXT doesn't contain the answer truthfully say \"I don't know\".\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "\n",
    "template_it = \"\"\"\n",
    "Answer the following QUESTION based on the CONTEXT given in Italian language. If you do not know the answer and the CONTEXT doesn't contain the answer truthfully say \"I don't know\".\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "def format_messages(messages: List[Dict[str, str]]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Format messages for Llama-2 chat models.\n",
    "    \n",
    "    The model only supports 'system', 'user' and 'assistant' roles, starting with 'system', then 'user' and \n",
    "    alternating (u/a/u/a/u...). The last message must be from 'user'.\n",
    "    \"\"\"\n",
    "    prompt: List[str] = []\n",
    "\n",
    "    if messages[0][\"role\"] == \"system\":\n",
    "        content = \"\".join([\"<<SYS>>\\n\", messages[0][\"content\"], \"\\n<</SYS>>\\n\\n\", messages[1][\"content\"]])\n",
    "        messages = [{\"role\": messages[1][\"role\"], \"content\": content}] + messages[2:]\n",
    "    for user, answer in zip(messages[::2], messages[1::2]):\n",
    "        prompt.extend([\"<s>\", \"[INST] \", (user[\"content\"]).strip(), \" [/INST] \", (answer[\"content\"]).strip(), \"</s>\"])\n",
    "    prompt.extend([\"<s>\", \"[INST] \", (messages[-1][\"content\"]).strip(), \" [/INST] \"])\n",
    "    return \"\".join(prompt)\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt, model_kwargs):\n",
    "        base_input = [{\"role\" : \"user\", \"content\" : prompt}]\n",
    "        optz_input = format_messages(base_input)\n",
    "        input_str = json.dumps({\n",
    "            \"inputs\" : optz_input, \n",
    "            \"parameters\" : {**model_kwargs}\n",
    "        })\n",
    "        return input_str.encode('utf-8')\n",
    "    \n",
    "    def transform_output(self, output):\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[\"generated_text\"]\n",
    "    \n",
    "content_handler = ContentHandler()\n",
    "\n",
    "llm=SagemakerEndpoint(\n",
    "             endpoint_name=llm_endpoint_name, \n",
    "             region_name=aws_region, \n",
    "             model_kwargs={\"max_new_tokens\": 400, \"top_p\": 1.0, \"temperature\": 0.1},\n",
    "             endpoint_kwargs={\"CustomAttributes\": \"accept_eula=true\"},\n",
    "             content_handler=content_handler\n",
    "         )\n",
    "\n",
    "llm_qa_smep_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type='stuff',\n",
    "    retriever=docsearch.as_retriever(search_kwargs={\n",
    "        \"k\": 3, \n",
    "        \"space_type\": \"cosineSimilarity\",\n",
    "        \"space_type\": \"painless_scripting\"\n",
    "    }),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt_template}\n",
    ")\n",
    "\n",
    "def pretty_print(chain_op):\n",
    "    question = chain_op['query']\n",
    "    response = chain_op['result']\n",
    "    sources = \"-\" + \"\\n-\".join([f\"{src.metadata['source'].split('/')[-1]} (page: {src.metadata['page']})\" for src in chain_op['source_documents']])\n",
    "    sources = f\"\"\"```bash{sources}\"\"\"\n",
    "    stdout = f\"\"\"{response}\\n\\n##### Sources:\\n{sources}\"\"\"\n",
    "    return stdout\n",
    "\n",
    "llm_qa_smep_chain(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e218d0a-efeb-4d60-810c-536b29536e72",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "\n",
    "Delete the SageMaker Inference Endpoints that we created in this notebook to avoid incurring future costs. If you created an Amazon OpenSearch Serverless Collection for this example and no longer require it then delete it via the AWS Console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95c2ad5-4bd2-4014-a86c-218514aa216b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete LLM\n",
    "llm_predictor.delete_model()\n",
    "llm_predictor.delete_predictor(delete_endpoint_config=True)\n",
    "\n",
    "# Delete Embeddings Model\n",
    "embed_predictor.delete_model()\n",
    "embed_predictor.delete_predictor(delete_endpoint_config=True)\n",
    "\n",
    "# Delete OpenSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e2b81f-a0fc-4e2c-8f93-7930b0b6d25d",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee53d433-3bfb-46da-935d-9a7c3a8a3929",
   "metadata": {},
   "source": [
    "In this notebook, we used Retrieval Augmented Generation(RAG) as an optional approach to provide domain specific context to Large Language Models(LLM). We showed how to use SageMaker Jumpstart to easily build a RAG-based contextual chatbot for a financial services organization using Llama2 and Amazon OpenSearch Serverless as the Vector datastore. This method refines text generation using Llama2 by dynamically sourcing relevant context."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
